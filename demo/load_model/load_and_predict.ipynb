{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-bert.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "uA4moo4dpLBr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# keras-bert prediction with Cloud TPU\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\" >\n",
        " <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/HighCWu/keras-bert-tpu/blob/master/demo/load_model/load_and_predict.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/HighCWu/keras-bert-tpu/blob/master/demo/load_model/load_and_predict.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "Uk6wtvQsP-3R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Install Dependences\n",
        "! pip install keras-bert-tpu -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GBFMypMHSHlt",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Download Pretrained Podel\n",
        "import os\n",
        "UPLOAD_TIME = '2018_11_03' #@param {type:\"string\"}\n",
        "BERT_MODEL = 'chinese_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "download_url = 'https://storage.googleapis.com/bert_models/{}/{}.zip'.format(UPLOAD_TIME,BERT_MODEL)\n",
        "zip_path = '{}.zip'.format(BERT_MODEL)\n",
        "! test -d $BERT_MODEL || (wget $download_url && unzip $zip_path)\n",
        "BERT_PRETRAINED_DIR = os.path.realpath(BERT_MODEL)\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IiaEW5laTN6z",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from keras_bert.loader import load_trained_model_from_checkpoint\n",
        "from keras_bert.bert import *\n",
        "config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "model = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True)\n",
        "\n",
        "model.summary(line_length=120)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WHn1rWFLuubx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "use_tpu=True # @param {type:\"boolean\"}\n",
        "if use_tpu:\n",
        "  assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; Maybe you should switch hardware accelerator to TPU for TPU support'\n",
        "  import tensorflow as tf\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "          tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
        "  )\n",
        "  model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "                      model, strategy=strategy)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Vrs_AqDe_RH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import codecs\n",
        "import numpy as np\n",
        "\n",
        "bsz = 8 # TPU batch size must be a mutiple of 8\n",
        "\n",
        "dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
        "\n",
        "tokens = ['[CLS]', '[MASK]', '[MASK]'] + list('是利用符号语言研究数量、结构、变化以及空间等概念的一门学科') + ['[SEP]']\n",
        "\n",
        "token_dict = {}\n",
        "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "token_dict_rev = {v: k for k, v in token_dict.items()}\n",
        "\n",
        "token_input = np.asarray([[token_dict[token] for token in tokens] + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "seg_input = np.asarray([[0] * len(tokens) + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "mask_input = np.asarray([[0, 1, 1] + [0] * (512 - 3) for i in range(bsz)])\n",
        "\n",
        "\n",
        "print(token_input[0][:len(tokens)])\n",
        "\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[0]\n",
        "predicts = np.argmax(predicts, axis=-1)\n",
        "print(predicts[0][:len(tokens)])\n",
        "print(list(map(lambda x: token_dict_rev[x], predicts[0][1:3])))\n",
        "\n",
        "\n",
        "sentence_1 = '数学是利用符号语言研究數量、结构、变化以及空间等概念的一門学科。'\n",
        "sentence_2 = '从某种角度看屬於形式科學的一種。'\n",
        "\n",
        "tokens = ['[CLS]'] + list(sentence_1) + ['[SEP]'] + list(sentence_2) + ['[SEP]']\n",
        "\n",
        "token_input = np.asarray([[token_dict[token] for token in tokens] + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "seg_input = np.asarray([[0] * (len(sentence_1) + 2) + [1] * (len(sentence_2) + 1) + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "mask_input = np.asarray([[0] * 512 for i in range(bsz)])\n",
        "\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[1]\n",
        "print('%s is random next: ' % sentence_2, bool(np.argmax(predicts, axis=-1)[0]))\n",
        "\n",
        "sentence_2 = '任何一个希尔伯特空间都有一族标准正交基。'\n",
        "\n",
        "tokens = ['[CLS]'] + list(sentence_1) + ['[SEP]'] + list(sentence_2) + ['[SEP]']\n",
        "\n",
        "token_input = np.asarray([[token_dict[token] for token in tokens] + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "seg_input = np.asarray([[0] * (len(sentence_1) + 2) + [1] * (len(sentence_2) + 1) + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "mask_input = np.asarray([[0] * 512 for i in range(bsz)])\n",
        "\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[1]\n",
        "print('%s is random next: ' % sentence_2, bool(np.argmax(predicts, axis=-1)[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}